---
name: migrations-creator-py
description: Database migrations creator skill for Alembic-based projects with a shared library architecture (two-repo pattern). Handles full lifecycle from migration creation to library versioning and release.
model: inherit
color: yellow
argument-hint: [description-of-migration-changes]
allowed-tools: Read, Write, Edit, Grep, Glob, Bash, Task
---

# Database Migrations Creator

You are a specialized database migrations skill with deep expertise in Alembic, SQLAlchemy, and the shared library architecture pattern. Your primary focus is creating database migrations and updating all related artifacts (entities, criterias, mappers, repositories, and tests) across a two-repository architecture.

## User Instruction

$ARGUMENTS

---

## Architecture Overview

You operate across **two repositories**:
- **db-migrator-job**: The Alembic migration runner that holds revision files and executes migrations
- **Common structure library**: A shared Python library that holds the actual migration implementations, domain entities, infrastructure entities, criterias, mappers, and repositories

### Technology Stack

- **Migrations**: Alembic 1.14.0+ for database schema versioning
- **ORM**: SQLAlchemy 2.0+ for relational databases (PostgreSQL)
- **Validation**: Pydantic v2 for domain entities
- **Database**: PostgreSQL with UUID primary keys
- **Testing**: Pytest with unittest.mock (MagicMock, patch)
- **Python Version**: 3.11+

---

## Step 0: Gather Requirements

Before doing anything, you MUST collect the following information from the user:

### 0.1 — Repository Paths

Ask the user for:
1. **Absolute path to the db-migrator-job repository** on their local machine
2. **Absolute path to the common structure library repository** on their local machine

These paths vary between team members. Example:
```
db-migrator-job:            /Users/dev/projects/db-migrator-job
common-structure-library:   /Users/dev/projects/common-structure-library
```

### 0.2 — Migration Description

Ask the user **what database changes they want to make**. They should describe:

| Change Type | What to Ask |
|---|---|
| **Create table** | Table name, all columns with types, nullability, defaults, foreign keys, and relationships |
| **Add column(s)** | Table name, column name(s), type(s), nullable?, default value? |
| **Remove column(s)** | Table name, column name(s) to remove |
| **Modify column(s)** | Table name, column name, what changes (type, nullable, default, etc.) |
| **Multiple changes** | Each change described separately |

For each column, clarify:
- **Type**: String(length), Text, Integer, Float, Boolean, DateTime, UUID, Enum (with values)
- **Nullability**: Required (`nullable=False`) or optional (`nullable=True`)
- **Default value**: Python-side default or `server_default`
- **Foreign key**: Target table and column (e.g., `users.id`)
- **Relationships**: Any ORM relationships to define

### 0.3 — Explore Existing Codebase

Before writing any code:
1. **In db-migrator-job**: Read `alembic/versions/` to find the latest migration and its revision ID (this becomes `down_revision`)
2. **In common structure library**: Read existing entities, criterias, mappers, and repositories to understand established patterns and ensure consistency

---

## Step 1: Create the Alembic Revision in db-migrator-job

1. Run the Alembic command to generate the migration file:
   ```bash
   cd [db-migrator-job-path]
   alembic revision -m "migration_name"
   ```

2. This generates a file in `alembic/versions/` with a unique revision ID (e.g., `af4073c70858_create_users_table.py`)

3. Verify the generated revision ID and `down_revision` (must point to the latest existing migration)

4. **Fix the filename if truncated**: Alembic may truncate long filenames. Rename to the full version for consistency.

---

## Step 2: Create the Migration Implementation in Common Structure Library

In the common structure library repository, create the migration file at:
```
common_structure_library/infrastructure/alembic_migrations/{migration_name}.py
```

### Migration File Pattern

```python
"""{migration_name}

Revision ID: {revision_id}
Revises: {down_revision_id}
Create Date: {timestamp}

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects.postgresql import UUID
from common_structure_library.infrastructure.alembic_migrations.alembic_common import _add_timestamp_columns

# revision identifiers, used by Alembic.
{migration_name}_revision = '{revision_id}'
{migration_name}_down_revision = '{down_revision_id}'
{migration_name}_branch_labels = None
{migration_name}_depends_on = None

branch_labels = {migration_name}_branch_labels
depends_on = {migration_name}_depends_on


def {migration_name}_upgrade() -> None:
    # Implementation here
    pass


def {migration_name}_downgrade() -> None:
    # Reverse implementation here
    pass
```

**CRITICAL naming rules**:
- All exported variables and functions MUST be prefixed with `{migration_name}_` to avoid conflicts when multiple migrations are imported together
- Module-level aliases `branch_labels` and `depends_on` must reference the prefixed versions

### Create Table Example

```python
def create_users_table_upgrade() -> None:
    op.create_table(
        'users',
        sa.Column('id', UUID(as_uuid=True), primary_key=True),
        sa.Column('first_name', sa.String(50), nullable=False),
        sa.Column('last_name', sa.String(50), nullable=False),
        sa.Column('email', sa.String(255), nullable=True),
        sa.Column('cellphone', sa.String(20), nullable=False),
        sa.Column('status', sa.Enum('ACTIVE', 'INACTIVE', name='user_status_enum'), nullable=False, server_default='ACTIVE'),
        sa.Column('charging_group_id', UUID(as_uuid=True), sa.ForeignKey('charging_groups.id'), nullable=True),
    )
    _add_timestamp_columns('users')


def create_users_table_downgrade() -> None:
    op.drop_table('users')
    # Drop enums created in upgrade
    sa.Enum(name='user_status_enum').drop(op.get_bind(), checkfirst=True)
```

### Add Column Example

```python
def add_provider_to_users_table_upgrade() -> None:
    op.add_column('users', sa.Column('provider', sa.String(50), nullable=False, server_default='phone'))


def add_provider_to_users_table_downgrade() -> None:
    op.drop_column('users', 'provider')
```

### Remove Column Example

```python
def remove_is_blocked_from_users_table_upgrade() -> None:
    op.drop_column('users', 'is_blocked')


def remove_is_blocked_from_users_table_downgrade() -> None:
    op.add_column('users', sa.Column('is_blocked', sa.Boolean, nullable=False, server_default=sa.text('false')))
```

### Modify Column Example

```python
def modify_email_in_users_table_upgrade() -> None:
    op.alter_column('users', 'email', existing_type=sa.String(255), nullable=False)


def modify_email_in_users_table_downgrade() -> None:
    op.alter_column('users', 'email', existing_type=sa.String(255), nullable=True)
```

### Index Rules

- **Automatically** create indexes for all foreign key columns
- Create indexes for columns that will be frequently queried (ask the user if unsure)
- Name indexes with the pattern `idx_{table}_{column}`

```python
op.create_index('idx_users_charging_group_id', 'users', ['charging_group_id'])
```

In downgrade, drop indexes before dropping columns/tables:
```python
op.drop_index('idx_users_charging_group_id', table_name='users')
```

### Enum Handling

When creating enums in a migration:
```python
# In upgrade — create enum explicitly before using in add_column
user_status_enum = sa.Enum('ACTIVE', 'INACTIVE', name='user_status_enum')
user_status_enum.create(op.get_bind(), checkfirst=True)
op.add_column('users', sa.Column('status', user_status_enum, nullable=False, server_default='ACTIVE'))

# In downgrade — drop enum after removing column
op.drop_column('users', 'status')
sa.Enum(name='user_status_enum').drop(op.get_bind(), checkfirst=True)
```

### Timestamp Columns

Always use the shared utility for timestamp columns when creating tables:
```python
from common_structure_library.infrastructure.alembic_migrations.alembic_common import _add_timestamp_columns

# After op.create_table(...)
_add_timestamp_columns('table_name')
```

This adds `created_at` and `updated_at` columns with `server_default=sa.text("(now() at time zone 'utc')")`.

---

## Step 3: Wire the Migration in db-migrator-job

In the migration file generated in Step 1 (`alembic/versions/{revision_id}_{migration_name}.py`), replace its content with:

```python
"""{migration_name}

Revision ID: {revision_id}
Revises: {down_revision_id}
Create Date: {timestamp}

"""
from common_structure_library.infrastructure.alembic_migrations.{migration_name} import (
    {migration_name}_revision,
    {migration_name}_down_revision,
    {migration_name}_upgrade,
    {migration_name}_downgrade
)

# revision identifiers, used by Alembic.
revision = {migration_name}_revision
down_revision = {migration_name}_down_revision
branch_labels = None
depends_on = None


def upgrade() -> None:
    {migration_name}_upgrade()


def downgrade() -> None:
    {migration_name}_downgrade()
```

This pattern delegates all logic to the common library while maintaining Alembic's standard interface.

---

## Step 4: Update Entities, Criterias, Mappers, Repositories, and Tests

When the migration modifies database tables (add/remove/modify columns or create new tables), you MUST update all related files in the common structure library.

### 4.1 — Search for Affected Files

Use grep/search by the table name and affected field names across these directories:

| Directory | Purpose | Naming Pattern |
|---|---|---|
| `domain/entities/` | Domain entities (Pydantic) | `{entity_name}_domain_entity.py` |
| `infrastructure/entities/` | Infra entities (SQLAlchemy) | `{entity_name}_infra_entity.py` |
| `domain/criterias/{entity}/` | Create/Update/Find/Filter criterias | `{action}_{entity}_criteria.py` |
| `infrastructure/mappers/` | Domain <-> Infra conversion | `{entity}_mapper.py` |
| `domain/repositories/` | Abstract repository interfaces | `{entity}_repository.py` |
| `infrastructure/repositories/` | PostgreSQL implementations | `postgres_{entity}_repository.py` |
| `tests/` | All corresponding tests | Mirror of source structure |

### 4.2 — For Existing Tables (Add/Remove/Modify Columns)

Apply changes to EVERY affected file:

- **Adding columns**: Add fields/parameters with correct types and defaults
- **Removing columns**: Remove fields/parameters from all files
- **Modifying columns**: Update type/nullability in all files

### 4.3 — For New Tables (Full File Creation)

When creating a brand new table, create the **complete set of files**:

#### Domain Entity

```python
# domain/entities/{entity_name}_domain_entity.py
from typing import Optional
from uuid import UUID
from pydantic import Field
from common_structure_library.domain.entities.base_domain_entity import BaseDomainEntity


class {Entity}DomainEntity(BaseDomainEntity):
    name: str = Field(...)
    description: Optional[str] = Field(default=None)
    # Add all fields matching the migration columns
    # Use Field(...) for required, Field(default=X) for optional

    def __repr__(self):
        return f"<{Entity}DomainEntity(id={self.id}, name={self.name})>"

    @staticmethod
    def get_table_name() -> str:
        return "{table_name}"
```

#### Infrastructure Entity

```python
# infrastructure/entities/{entity_name}_infra_entity.py
import uuid
from sqlalchemy import Column, String, Boolean, ForeignKey
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.orm import relationship
from common_structure_library.infrastructure.entities.base_infra_entity import BaseInfraEntity


class {Entity}InfraEntity(BaseInfraEntity):
    __tablename__ = "{table_name}"

    name = Column(String(100), nullable=False)
    description = Column(String(255), nullable=True)
    # Add all columns matching the migration
    # Foreign keys: Column(UUID(as_uuid=True), ForeignKey('other_table.id'), nullable=True)
    # Relationships: relationship("{OtherEntity}InfraEntity", back_populates="{this_table}")
```

#### Criterias

Create one file per action in `domain/criterias/{entity_name}/`:

```python
# Create criteria
class {Entity}CreateCriteria:
    def __init__(self, name: str, description: str = None):
        self.name = name
        self.description = description

# Find one criteria
class {Entity}FindOneCriteria:
    def __init__(self, name: str = None, id: UUID = None):
        self.name = name
        self.id = id

# Update criteria
class {Entity}UpdateCriteria:
    def __init__(self, name: str = None, description: str = None):
        self.name = name
        self.description = description

# Filter criteria (for find_all)
class {Entity}FilterCriteria:
    def __init__(self, name: str = None, description: str = None):
        self.name = name
        self.description = description
```

**IMPORTANT**: Criterias are **plain Python classes**, NOT Pydantic models. They use `__init__` with explicit parameters stored as `self.field = value`.

#### Mapper

```python
# infrastructure/mappers/{entity_name}_mapper.py
from typing import Optional, List
from common_structure_library.infrastructure.mappers.base_mapper_builder import BaseMapperBuilder
from common_structure_library.domain.entities.{entity_name}_domain_entity import {Entity}DomainEntity
from common_structure_library.infrastructure.entities.{entity_name}_infra_entity import {Entity}InfraEntity


class {Entity}Mapper(BaseMapperBuilder[{Entity}InfraEntity, {Entity}DomainEntity]):

    def _create_base_domain_entity(self) -> {Entity}DomainEntity:
        return {Entity}DomainEntity(
            id=self._infra_entity.id,
            name=self._infra_entity.name,
            description=self._infra_entity.description,
            created_at=self._infra_entity.created_at,
            updated_at=self._infra_entity.updated_at,
        )

    # Add with_{relationship}() methods for each relationship using Builder pattern:
    # def with_related_items(self) -> "{Entity}Mapper":
    #     if self._infra_entity.related_items:
    #         from ...related_mapper import RelatedMapper
    #         domain_entity = self._get_base_domain_entity()
    #         domain_entity.related_items = [
    #             RelatedMapper(item).build() for item in self._infra_entity.related_items
    #         ]
    #     return self

    @staticmethod
    def domain_entity_to_infra_entity(domain_entity: {Entity}DomainEntity) -> {Entity}InfraEntity:
        return {Entity}InfraEntity(
            id=domain_entity.id,
            name=domain_entity.name,
            description=domain_entity.description,
        )

    @staticmethod
    def infra_entities_to_domain_entities(infra_entities: List[{Entity}InfraEntity]) -> List[{Entity}DomainEntity]:
        return [{Entity}Mapper(ie).build() for ie in infra_entities]
```

#### Domain Repository Interface

```python
# domain/repositories/{entity_name}_repository.py
from abc import abstractmethod
from typing import Optional, List
from uuid import UUID
from common_structure_library.domain.repositories.base_repository import BaseRepository
from common_structure_library.domain.entities.{entity_name}_domain_entity import {Entity}DomainEntity
from common_structure_library.domain.criterias.{entity_name}.create_{entity_name}_criteria import {Entity}CreateCriteria
from common_structure_library.domain.criterias.{entity_name}.update_{entity_name}_criteria import {Entity}UpdateCriteria
from common_structure_library.domain.criterias.{entity_name}.filter_{entity_name}_criteria import {Entity}FilterCriteria
from common_structure_library.domain.criterias.find_one_by_id_criteria import FindOneByIdCriteria
from common_structure_library.domain.criterias.update_one_by_id_criteria import UpdateOneByIdCriteria
from common_structure_library.domain.criterias.delete_one_by_id_criteria import DeleteOneByIdCriteria


class {Entity}Repository(BaseRepository[
    {Entity}DomainEntity,
    {Entity}CreateCriteria,
    {Entity}UpdateCriteria,
    {Entity}FilterCriteria,
    FindOneByIdCriteria,
    UpdateOneByIdCriteria,
    DeleteOneByIdCriteria
]):
    @abstractmethod
    def find_one_by_attributes(self, criteria) -> Optional[{Entity}DomainEntity]:
        pass
```

#### PostgreSQL Repository Implementation

```python
# infrastructure/repositories/postgres_{entity_name}_repository.py
from typing import Optional, List
from sqlalchemy.orm import Session
from common_structure_library.infrastructure.repositories.base_postgres_repository import BasePostgresRepository
from common_structure_library.domain.repositories.{entity_name}_repository import {Entity}Repository
from common_structure_library.infrastructure.entities.{entity_name}_infra_entity import {Entity}InfraEntity
from common_structure_library.infrastructure.mappers.{entity_name}_mapper import {Entity}Mapper
from common_structure_library.domain.entities.{entity_name}_domain_entity import {Entity}DomainEntity


class Postgres{Entity}Repository(BasePostgresRepository, {Entity}Repository):

    def create_one(self, criteria) -> {Entity}DomainEntity:
        infra_entity = {Entity}InfraEntity(
            name=criteria.name,
            description=criteria.description,
        )
        self.db_session.add(infra_entity)
        self.db_session.commit()
        self.db_session.refresh(infra_entity)
        return {Entity}Mapper(infra_entity).build()

    def find_one_by_id(self, criteria) -> Optional[{Entity}DomainEntity]:
        infra_entity = self.db_session.query({Entity}InfraEntity).filter(
            {Entity}InfraEntity.id == criteria.id
        ).first()
        if infra_entity is None:
            return None
        return {Entity}Mapper(infra_entity).build()

    def find_one_by_attributes(self, criteria) -> Optional[{Entity}DomainEntity]:
        query = self.db_session.query({Entity}InfraEntity)
        if criteria.name is not None:
            query = query.filter({Entity}InfraEntity.name == criteria.name)
        result = query.first()
        if result is None:
            return None
        return {Entity}Mapper(result).build()

    def find_all_by_attributes(self, criteria) -> List[{Entity}DomainEntity]:
        query = self.db_session.query({Entity}InfraEntity)
        if criteria.name is not None:
            query = query.filter({Entity}InfraEntity.name == criteria.name)
        results = query.all()
        return {Entity}Mapper.infra_entities_to_domain_entities(results)

    def update_one(self, criteria) -> Optional[{Entity}DomainEntity]:
        infra_entity = self.db_session.query({Entity}InfraEntity).filter(
            {Entity}InfraEntity.id == criteria.id
        ).first()
        if infra_entity is None:
            return None
        for field in ['name', 'description']:
            value = getattr(criteria, field, None)
            if value is not None:
                setattr(infra_entity, field, value)
        self.db_session.commit()
        self.db_session.refresh(infra_entity)
        return {Entity}Mapper(infra_entity).build()

    def delete_one(self, criteria) -> bool:
        infra_entity = self.db_session.query({Entity}InfraEntity).filter(
            {Entity}InfraEntity.id == criteria.id
        ).first()
        if infra_entity is None:
            return False
        self.db_session.delete(infra_entity)
        self.db_session.commit()
        return True
```

### 4.4 — Update `__init__.py` Exports

After creating or modifying files, update the corresponding `__init__.py` files to re-export all public classes:

**Domain entities `__init__.py`**: Import all entities and call `model_rebuild()` for Pydantic v2 forward reference resolution:
```python
from .{entity_name}_domain_entity import {Entity}DomainEntity
# ... other imports ...
{Entity}DomainEntity.model_rebuild()
```

**Per-entity criterias `__init__.py`**: Re-export with `__all__`:
```python
from .create_{entity_name}_criteria import {Entity}CreateCriteria
from .find_one_{entity_name}_criteria import {Entity}FindOneCriteria
from .update_{entity_name}_criteria import {Entity}UpdateCriteria
__all__ = ["{Entity}CreateCriteria", "{Entity}FindOneCriteria", "{Entity}UpdateCriteria"]
```

**Root criterias `__init__.py`**: Add the new entity's criterias to the existing imports and `__all__` list.

**Infrastructure entities `__init__.py`**: Import the new infra entity so SQLAlchemy registers it.

**Infrastructure mappers `__init__.py`**: Re-export the new mapper.

### 4.5 — Create Tests

Create tests for every new or modified file following the project's test structure:

**Test directory structure** (mirrors source):
```
tests/
├── domain/
│   ├── entities/{entity_name}/
│   │   └── test_{entity_name}_domain_entity.py
│   ├── criterias/{entity_name}/
│   │   ├── test_create_{entity_name}_criteria.py
│   │   ├── test_find_one_{entity_name}_criteria.py
│   │   └── test_update_{entity_name}_criteria.py
├── infrastructure/
│   ├── mappers/{entity_name}/
│   │   └── test_{entity_name}_mapper.py
│   └── repositories/postgres_{entity_name}_repository/
│       ├── test_create_one_from_postgres_{entity_name}_repository.py
│       ├── test_find_one_by_id_from_postgres_{entity_name}_repository.py
│       ├── test_find_one_by_attributes_from_postgres_{entity_name}_repository.py
│       ├── test_find_all_by_attributes_from_postgres_{entity_name}_repository.py
│       ├── test_update_one_from_postgres_{entity_name}_repository.py
│       └── test_delete_one_from_postgres_{entity_name}_repository.py
```

**Test naming**: Each repository method gets its own file: `test_{method_name}_from_postgres_{entity_name}_repository.py`

**Test pattern** (Arrange/Act/Assert with MagicMock):

```python
from unittest.mock import MagicMock, patch
from common_structure_library.infrastructure.repositories.postgres_{entity_name}_repository import Postgres{Entity}Repository
from common_structure_library.domain.criterias.{entity_name}.create_{entity_name}_criteria import {Entity}CreateCriteria


class TestCreateOneFromPostgres{Entity}Repository:
    def test_create_one_should_create_entity_with_all_fields(self):
        # Arrange
        db_session = MagicMock()
        repository = Postgres{Entity}Repository(db_session)
        criteria = {Entity}CreateCriteria(name="Test", description="Test description")

        with patch(
            'common_structure_library.infrastructure.repositories.postgres_{entity_name}_repository.{Entity}InfraEntity'
        ) as MockInfraEntity, patch(
            'common_structure_library.infrastructure.repositories.postgres_{entity_name}_repository.{Entity}Mapper'
        ) as MockMapper:
            mock_infra_instance = MagicMock()
            MockInfraEntity.return_value = mock_infra_instance
            mock_mapper_instance = MagicMock()
            MockMapper.return_value = mock_mapper_instance
            mock_domain_entity = MagicMock()
            mock_mapper_instance.build.return_value = mock_domain_entity

            # Act
            result = repository.create_one(criteria)

            # Assert
            MockInfraEntity.assert_called_once_with(
                name="Test",
                description="Test description",
            )
            db_session.add.assert_called_once_with(mock_infra_instance)
            db_session.commit.assert_called_once()
            db_session.refresh.assert_called_once_with(mock_infra_instance)
            assert result == mock_domain_entity
```

### 4.6 — Run Tests

After all changes, run the test suite in the common structure library:

```bash
cd [common-structure-library-path]
coverage run -m pytest -rP && coverage report -m
```

Verify that all tests pass and coverage is acceptable before proceeding.

---

## Step 5: Version and Release the Common Structure Library

1. **Bump the version** in `common_structure_library/_version.py`, incrementing according to semantic versioning:
   - **Patch** (X.Y.Z -> X.Y.Z+1): Adding columns, modifying columns
   - **Minor** (X.Y.Z -> X.Y+1.0): Creating new tables with full entity/repo support
   - **Major**: Breaking changes to existing interfaces

2. **Create the release branch** (if not already on one):
   ```bash
   cd [common-structure-library-path]
   git checkout -b release/v.X.Y.Z
   ```

3. **Commit and push**:
   ```bash
   git add .
   git commit -m "v.X.Y.Z - brief description of changes"
   git push -u origin release/v.X.Y.Z
   ```

4. **Create a PR** towards `main`:
   ```bash
   gh pr create --title "v.X.Y.Z - brief description" --body "Migration changes description"
   ```

5. **Update `requirements.txt`** in db-migrator-job with the new version tag:
   ```
   git+https://...@github.com/{org}/common-structure-library.git@v.X.Y.Z#egg=common-structure-library
   ```

---

## Step 6: Run Tests and Post Results to the PR

1. Run the full test suite with coverage in common-structure-library:
   ```bash
   cd [common-structure-library-path]
   coverage run -m pytest -rP && coverage report -m
   ```

2. Verify all tests pass and coverage is acceptable.

3. Post the results as a comment on the PR:
   ```bash
   gh pr comment [PR_NUMBER] --body "## Test Results

   \`\`\`
   [paste pytest summary: X passed, Y skipped, Z warnings]
   \`\`\`

   ## Coverage Report -- XX.XX%

   | File | Stmts | Miss | Branch | BrPart | Cover |
   |------|-------|------|--------|--------|-------|
   | [coverage report rows] |
   | **TOTAL** | **X** | **X** | **X** | **X** | **XX.XX%** |
   "
   ```

---

## Step 7: Verify the Migration

1. Install updated dependencies in db-migrator-job:
   ```bash
   cd [db-migrator-job-path]
   pip install -r requirements.txt --upgrade
   ```

2. Check migration state:
   ```bash
   alembic current
   alembic history
   ```

3. Run the migration:
   ```bash
   alembic upgrade head
   ```

4. If errors occur, fix them in the common structure library and repeat from Step 2.

---

## Migration Naming Conventions

Use `alembic revision -m "message"` with these naming patterns:

| Operation | Message Format |
|---|---|
| Add column | `add_{attribute_name}_to_{table_name}_table` |
| Remove column | `remove_{attribute_name}_from_{table_name}_table` |
| Modify column | `modify_{attribute_name}_in_{table_name}_table` |
| Create table | `create_{table_name}_table` |
| Drop table | `remove_{table_name}_table` |
| Multiple changes | `add_{x}_and_{y}_remove_{z}_in_{table_name}_table` |

---

## Data Type Reference

| Type | SQLAlchemy Column | Notes |
|---|---|---|
| UUID | `UUID(as_uuid=True)` | Import from `sqlalchemy.dialects.postgresql`. Always used for primary keys and foreign keys. |
| String | `sa.String(length=N)` | Always specify length. |
| Text | `sa.Text` | For long text without length limit. |
| Boolean | `sa.Boolean` | |
| Integer | `sa.Integer` | |
| Float | `sa.Float` | |
| DateTime | `sa.DateTime` | Use `_add_timestamp_columns()` for `created_at`/`updated_at`. |
| Enum | `sa.Enum('V1', 'V2', name='enum_name')` | Create with `enum.create(op.get_bind(), checkfirst=True)` in upgrade, drop in downgrade. |
| Foreign Key | `sa.ForeignKey('table.column')` | Always add an index on FK columns. |

**Nullability shorthand**:
- Required field -> `nullable=False`
- Optional field -> `nullable=True`

---

## Anti-Patterns to Avoid

### Migration Anti-Patterns

1. **Missing downgrade implementation**: Every `upgrade()` MUST have a corresponding `downgrade()` that fully reverses the changes
2. **Broken revision chain**: Always verify `down_revision` points to the actual latest migration -- never hardcode or guess
3. **Multiple logical changes in one migration**: Each migration should represent ONE logical change. Split unrelated changes into separate migrations
4. **Missing indexes on foreign keys**: Always create indexes for FK columns -- this is done automatically by this skill
5. **Forgetting to drop enums in downgrade**: When `upgrade()` creates an enum, `downgrade()` must drop it
6. **Using raw SQL strings**: Use Alembic operations (`op.create_table`, `op.add_column`, etc.), not raw SQL
7. **Not using `_add_timestamp_columns()`**: Always use the shared utility for `created_at`/`updated_at` -- never create these columns manually
8. **Forgetting `server_default` for NOT NULL columns on existing tables**: When adding a `nullable=False` column to an existing table, you MUST provide a `server_default` or the migration will fail on tables with existing rows

### Library Update Anti-Patterns

1. **Updating migration but not entities**: If the migration adds a column, the domain entity, infra entity, criterias, mapper, and repository MUST all be updated
2. **Forgetting `__init__.py` exports**: New files must be re-exported through their `__init__.py`
3. **Using Pydantic for criterias**: Criterias are plain Python classes, NOT Pydantic models
4. **Skipping `model_rebuild()`**: Domain entities with forward references need `model_rebuild()` in their `__init__.py`
5. **Missing tests**: Every new or modified file must have corresponding tests
6. **Not running tests before committing**: Always run `coverage run -m pytest -rP && coverage report -m` to verify no regressions

---

## Downgrade Safety Checklist

Before finalizing any migration, verify:

- [ ] `downgrade()` fully reverses `upgrade()` -- if upgrade creates a table, downgrade drops it
- [ ] Indexes are dropped BEFORE columns/tables in downgrade
- [ ] Enums are dropped AFTER the column using them is removed in downgrade
- [ ] Foreign key constraints are handled in correct order (child tables before parent tables)
- [ ] Running `upgrade` -> `downgrade` -> `upgrade` produces the same result (idempotent cycle)
